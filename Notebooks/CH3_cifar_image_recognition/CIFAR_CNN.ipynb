{"cells":[{"cell_type":"markdown","source":"# Image recognition on the CIFAR-10 dataset","metadata":{"tags":[],"cell_id":"6ac71bd2b0324ab4a2c8dbd29e28251b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"we need to recognize images containing objects from the CIFAR dataset. It contains 60k labelled images of 10 different objects. \n\n![Picture title](https://miro.medium.com/max/505/1*r8S5tF_6naagKOnlIcGXoQ.png)\n","metadata":{"tags":[],"cell_id":"dc152629129f488e89a8682de4c3395a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Import libraries and define symbolic constants","metadata":{"tags":[],"cell_id":"0371674ff3bc43b4bf08686f11c1b522","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"### Install wandb to keep track of model performance","metadata":{"tags":[],"cell_id":"b482afa637164b6d85a99c667cb122df","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"!pip install --upgrade wandb\n!wandb login ff97f4ffa6b4b35ec56fc229fc572b0ba72ac1fb","metadata":{"tags":[],"cell_id":"fa1b9dd4b17341ecb962ce956c784136","source_hash":"10f68b6d","execution_start":1668277917315,"execution_millis":16366,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /root/venv/lib/python3.10/site-packages (0.13.5)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.10/site-packages (from wandb) (63.2.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /root/venv/lib/python3.10/site-packages (from wandb) (1.10.1)\nRequirement already satisfied: GitPython>=1.0.0 in /root/venv/lib/python3.10/site-packages (from wandb) (3.1.29)\nRequirement already satisfied: six>=1.13.0 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: PyYAML in /root/venv/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /root/venv/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: pathtools in /root/venv/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: psutil>=5.0.0 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: promise<3,>=2.0 in /root/venv/lib/python3.10/site-packages (from wandb) (2.3)\nRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from wandb) (3.19.6)\nRequirement already satisfied: requests<3,>=2.0.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: shortuuid>=0.5.0 in /root/venv/lib/python3.10/site-packages (from wandb) (1.0.10)\nRequirement already satisfied: setproctitle in /root/venv/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /root/venv/lib/python3.10/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: smmap<6,>=3.0.1 in /root/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Import and initialize parameters","metadata":{"tags":[],"cell_id":"2dcccb0a3dba4440a1ac72e6e8e12055","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport wandb\nfrom wandb.keras import WandbCallback\n\nwandb.init(project=\"image-classification-cifar10\")\n\n\nIMG_CHANNELS = 3\nIMG_ROWS = 32\nIMG_COLUMNS = 32\nINPUT_SHAPE = (IMG_ROWS, IMG_COLUMNS, IMG_CHANNELS)\n\nEPOCHS = 40  # this is how many times re-train the model, each time optimizing its weight and biases\nBATCH_SIZE = 64 # this is the number of instances we take from the training set before running the optimizer\nVERBOSE = 1 # make it loud\nN_CLASSES = 10 # 10 classes in the cifar dataset\nVALIDATION_SPLIT = 0.2 # 20% of the training data for the validation\n\n#----------------HYPERPARAMETERS-----------------\nCONVOLUTION_FILTERS_SIZE = (3,3)\nPOOL_SIZE = (2,2)\nN_HIDDEN = 512 # neurons in hidden dense layer\nDROPOUT = 0.25 # portion of dropout values in the network  \nDROPOUT_DEEP = 0.5 # portion of dropout values in the network \nACTIVATION_FUNCTION_HIDDEN = 'relu' # activation function for the hidden layers\nACTIVATION_FUNCTION_FINAL = 'softmax' # activation function for the output layer \nOPTIMIZER = 'RMSprop' # optimizer, this is how we search for the minimum in the loss function\nLOSS_FUNCTION = 'categorical_crossentropy' #loss function, this is what is otimized\nMETRICS = ['accuracy'] #Our metrics, used to make sure we don't overfit. Computed also on the test set \n\nwandb.config = {\n  \"batch_size\": BATCH_SIZE, \n  \"n_hidden\": N_HIDDEN,\n  'activation_funciton_hidden': ACTIVATION_FUNCTION_HIDDEN,\n  'activation_funciton_final': ACTIVATION_FUNCTION_FINAL,\n  'optimizer': OPTIMIZER,\n  'loss_function': LOSS_FUNCTION,\n  'metric': METRICS,\n}\n","metadata":{"tags":[],"cell_id":"caf4b63776e542bab2395b66cdd2ee01","source_hash":"8416d941","execution_start":1668277933687,"execution_millis":22225,"is_output_hidden":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-11-12 18:32:13.702874: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-12 18:32:13.819134: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-11-12 18:32:13.824232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-11-12 18:32:13.824249: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-11-12 18:32:13.849331: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-11-12 18:32:15.934245: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-11-12 18:32:15.934300: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-11-12 18:32:15.934306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.5"},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/work/wandb/run-20221112_183227-18cp0wz1</code>"},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/picklerick/image-classification-cifar10/runs/18cp0wz1\" target=\"_blank\">eager-pine-7</a></strong> to <a href=\"https://wandb.ai/picklerick/image-classification-cifar10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{},"output_type":"display_data"}],"execution_count":2},{"cell_type":"markdown","source":"## Load dataset from Keras\r","metadata":{"tags":[],"cell_id":"474b0ef90fbf42d7bc729d1f7d8bf21e","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"def load_data():\n    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n \n    #normalize \n    mean = np.mean(x_train,axis=(0,1,2,3))\n    std = np.std(x_train,axis=(0,1,2,3))\n    x_train = (x_train-mean)/(std+1e-7)\n    x_test = (x_test-mean)/(std+1e-7)\n \n    y_train =  tf.keras.utils.to_categorical(y_train,N_CLASSES)\n    y_test =  tf.keras.utils.to_categorical(y_test,N_CLASSES)\n\n    return x_train, y_train, x_test, y_test\n\nx_train, y_train, x_test, y_test = load_data()\n\n#Augmet the dataset\ndatagen = ImageDataGenerator(\n    rotation_range = 30,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    zoom_range =  0.2,\n    horizontal_flip = True, \n)\n\ndatagen.fit(x_train)","metadata":{"tags":[],"cell_id":"ae16adbc6b164a808fc7f40c48921556","source_hash":"e2c323a4","execution_start":1668277955910,"execution_millis":17254,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 13s 0us/step\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Build the model","metadata":{"tags":[],"cell_id":"aaedb3a3b8684ac387d61eeaa0c9d138","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"- We use typical CNN architecture with convolutions and pooling","metadata":{"tags":[],"cell_id":"2eb0d334a5e445e193747893e032f090","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We make the network deeper. This increases complexity, so we need batch normalization and dropouts. ","metadata":{"tags":[],"cell_id":"76fc35d2-7474-40e5-8a0f-259637fdac77","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- As we go deeper, the number of neurons in the network increases. ","metadata":{"tags":[],"cell_id":"0ac575aa-0af1-40e4-a0f8-8f94ce7f9b45","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We then flatten the outputs of the convolution and pass them to a dense layer with a softmax activation function","metadata":{"tags":[],"cell_id":"8a399d92-7d1d-475d-be9e-96b32ee8587c","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":92,"fromCodePoint":85}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"def build_model():\n    model = models.Sequential()\n\n#1st block \n    model.add(\n        layers.Convolution2D(\n            IMG_ROWS,\n            CONVOLUTION_FILTERS_SIZE,\n            padding='same',\n            activation=ACTIVATION_FUNCTION_HIDDEN, \n            input_shape=INPUT_SHAPE\n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(\n        layers.Convolution2D(\n            32,\n            CONVOLUTION_FILTERS_SIZE,\n            padding='same',\n            activation=ACTIVATION_FUNCTION_HIDDEN, \n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(\n        layers.MaxPooling2D(pool_size=POOL_SIZE)\n    )\n    model.add(\n        layers.Dropout(DROPOUT)\n    )\n\n    #2nd layer\n    model.add(\n        layers.Convolution2D(\n            64,\n            CONVOLUTION_FILTERS_SIZE,\n            padding='same',\n            activation=ACTIVATION_FUNCTION_HIDDEN, \n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(\n        layers.Convolution2D(\n            64,\n            CONVOLUTION_FILTERS_SIZE,\n            padding='same',\n            activation=ACTIVATION_FUNCTION_HIDDEN, \n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(\n        layers.MaxPooling2D(pool_size=POOL_SIZE)\n    )\n    model.add(\n        layers.Dropout(0.3)\n    )\n\n    #3rd layer\n    model.add(\n        layers.Convolution2D(\n            128,\n            CONVOLUTION_FILTERS_SIZE,\n            padding='same',\n            activation=ACTIVATION_FUNCTION_HIDDEN, \n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(\n        layers.Convolution2D(\n            128,\n            CONVOLUTION_FILTERS_SIZE,\n            padding='same',\n            activation=ACTIVATION_FUNCTION_HIDDEN, \n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(\n        layers.MaxPooling2D(pool_size=POOL_SIZE)\n    )\n    model.add(\n        layers.Dropout(0.4)\n    )\n\n    #flatten and feed to a softmax layer with N_classes neurons\n    model.add(\n        layers.Flatten()\n    )   \n    model.add(\n        layers.Dense(\n            N_CLASSES,\n            activation = ACTIVATION_FUNCTION_FINAL\n        ) \n    )\n\n    return model\n    \nmodel = build_model()\nmodel.summary()","metadata":{"tags":[],"cell_id":"eb49260189ee4153a9e27d6efd4e83f2","source_hash":"8d3cabe6","execution_start":1668277973165,"execution_millis":882,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization (BatchN  (None, 32, 32, 32)       128       \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n                                                                 \n batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n hNormalization)                                                 \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n )                                                               \n                                                                 \n dropout (Dropout)           (None, 16, 16, 32)        0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n2022-11-12 18:32:53.099816: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-11-12 18:32:53.099866: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-11-12 18:32:53.099887: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-ba4822a4-198a-4cdb-8280-0ca8d044b999): /proc/driver/nvidia/version does not exist\n2022-11-12 18:32:53.100144: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n                                                                 \n batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n                                                                 \n batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n 2D)                                                             \n                                                                 \n dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n                                                                 \n batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n hNormalization)                                                 \n                                                                 \n conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n                                                                 \n batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n 2D)                                                             \n                                                                 \n dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n dense (Dense)               (None, 10)                20490     \n                                                                 \n=================================================================\nTotal params: 309,290\nTrainable params: 308,394\nNon-trainable params: 896\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Compile the model","metadata":{"tags":[],"cell_id":"31307f755bbc4b638aeab345b8976080","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"- We use RMSprop as optimizer","metadata":{"tags":[],"cell_id":"c6f4e74fd91d468b8a968ac04f755104","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":15,"fromCodePoint":7}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- The loss function is categorical cross-entropy, this is particularly well-suited for multi-class problems with a one-hot encoding ","metadata":{"tags":[],"cell_id":"b026585b0588416b8c5cecf037ae3246","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We use accuracy to evaluate the performance of the model","metadata":{"tags":[],"cell_id":"94a97dbf25a541c18b1c8eef63540654","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"model.compile(\n    optimizer=OPTIMIZER,\n    loss=LOSS_FUNCTION,\n    metrics=METRICS\n)","metadata":{"tags":[],"cell_id":"26d02f54d0164e5f862d65fe7adbbaac","source_hash":"c27cdea0","execution_start":1668277973414,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Train the model","metadata":{"tags":[],"cell_id":"d9a0c27b441549ecaa2ac967e6230f6f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"We are now ready to train the model. We need to define the number of epochs and the batch size. ","metadata":{"tags":[],"cell_id":"7ae30d7a9ee9412c91a58c0eb5d068d7","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- Epochs are the number of times the model is exposed to the training dataset. Each time, it will run the optimizer (SGD) and try to minimize the loss function. ","metadata":{"tags":[],"cell_id":"a67b3790d94d46a2a50c9ded7a425c65","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":7,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Batch_size is the number of instances that the optimizer observes before tuning the weights and biases. There are many batches per epoch.","metadata":{"tags":[],"cell_id":"9b7196d69bd24183852bec1348eafa30","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":10,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We split the training data in an 80% training and 20% validation per epoch. The validation set is used to compute the metric and tune hyperparameters, to avoid overfitting.","metadata":{"tags":[],"cell_id":"92738e0d87c44594bbb1c91afc6c31eb","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We add early stopping, on the loss function on the validation set, with a patience of N epoch. This will stop the optimization if the loss function does not go down for N  consecutive epochs. ","metadata":{"tags":[],"cell_id":"2e8f293ff4004b62970101cd3e3e259b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"score = model.fit_generator(\n    datagen.flow(\n        x_train,\n        y_train,\n        batch_size=BATCH_SIZE\n    ),\n    epochs=EPOCHS,\n    validation_data=(x_test,y_test),\n    callbacks=[WandbCallback()]\n    )","metadata":{"tags":[],"cell_id":"107c5e6b53bc4dcf8b81ecf82f4eae4f","source_hash":"1581cbfa","execution_start":1668278205006,"execution_millis":9725795,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/40\n/tmp/ipykernel_263/4154025429.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  score = model.fit_generator(\n782/782 [==============================] - ETA: 0s - loss: 2.1403 - accuracy: 0.3494WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 243s 310ms/step - loss: 2.1403 - accuracy: 0.3494 - val_loss: 1.4646 - val_accuracy: 0.4969\nEpoch 2/40\n782/782 [==============================] - 238s 304ms/step - loss: 1.6678 - accuracy: 0.4650 - val_loss: 1.6226 - val_accuracy: 0.5118\nEpoch 3/40\n782/782 [==============================] - ETA: 0s - loss: 1.4367 - accuracy: 0.5228WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 242s 310ms/step - loss: 1.4367 - accuracy: 0.5228 - val_loss: 1.4122 - val_accuracy: 0.5939\nEpoch 4/40\n782/782 [==============================] - ETA: 0s - loss: 1.2802 - accuracy: 0.5658WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 244s 313ms/step - loss: 1.2802 - accuracy: 0.5658 - val_loss: 1.0976 - val_accuracy: 0.6331\nEpoch 5/40\n782/782 [==============================] - ETA: 0s - loss: 1.1630 - accuracy: 0.6001WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 245s 314ms/step - loss: 1.1630 - accuracy: 0.6001 - val_loss: 0.8987 - val_accuracy: 0.6917\nEpoch 6/40\n782/782 [==============================] - 242s 310ms/step - loss: 1.0792 - accuracy: 0.6281 - val_loss: 1.0507 - val_accuracy: 0.6660\nEpoch 7/40\n782/782 [==============================] - 244s 312ms/step - loss: 1.0275 - accuracy: 0.6454 - val_loss: 0.9749 - val_accuracy: 0.6857\nEpoch 8/40\n782/782 [==============================] - ETA: 0s - loss: 0.9795 - accuracy: 0.6643WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 245s 313ms/step - loss: 0.9795 - accuracy: 0.6643 - val_loss: 0.8490 - val_accuracy: 0.7221\nEpoch 9/40\n782/782 [==============================] - 237s 303ms/step - loss: 0.9414 - accuracy: 0.6739 - val_loss: 0.9219 - val_accuracy: 0.7079\nEpoch 10/40\n782/782 [==============================] - ETA: 0s - loss: 0.9128 - accuracy: 0.6844WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 242s 309ms/step - loss: 0.9128 - accuracy: 0.6844 - val_loss: 0.8035 - val_accuracy: 0.7414\nEpoch 11/40\n782/782 [==============================] - 240s 307ms/step - loss: 0.8908 - accuracy: 0.6905 - val_loss: 0.8067 - val_accuracy: 0.7360\nEpoch 12/40\n782/782 [==============================] - ETA: 0s - loss: 0.8608 - accuracy: 0.7047WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 250s 320ms/step - loss: 0.8608 - accuracy: 0.7047 - val_loss: 0.7434 - val_accuracy: 0.7583\nEpoch 13/40\n782/782 [==============================] - 243s 311ms/step - loss: 0.8462 - accuracy: 0.7083 - val_loss: 0.7566 - val_accuracy: 0.7550\nEpoch 14/40\n782/782 [==============================] - 244s 312ms/step - loss: 0.8284 - accuracy: 0.7112 - val_loss: 0.7601 - val_accuracy: 0.7512\nEpoch 15/40\n782/782 [==============================] - ETA: 0s - loss: 0.8087 - accuracy: 0.7211WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 256s 327ms/step - loss: 0.8087 - accuracy: 0.7211 - val_loss: 0.6657 - val_accuracy: 0.7793\nEpoch 16/40\n782/782 [==============================] - 243s 311ms/step - loss: 0.7925 - accuracy: 0.7271 - val_loss: 0.6795 - val_accuracy: 0.7765\nEpoch 17/40\n782/782 [==============================] - 238s 304ms/step - loss: 0.7829 - accuracy: 0.7291 - val_loss: 0.6778 - val_accuracy: 0.7794\nEpoch 18/40\n782/782 [==============================] - ETA: 0s - loss: 0.7694 - accuracy: 0.7328WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 240s 307ms/step - loss: 0.7694 - accuracy: 0.7328 - val_loss: 0.6525 - val_accuracy: 0.7898\nEpoch 19/40\n782/782 [==============================] - 240s 307ms/step - loss: 0.7513 - accuracy: 0.7416 - val_loss: 0.7310 - val_accuracy: 0.7681\nEpoch 20/40\n782/782 [==============================] - ETA: 0s - loss: 0.7484 - accuracy: 0.7384WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 253s 323ms/step - loss: 0.7484 - accuracy: 0.7384 - val_loss: 0.6381 - val_accuracy: 0.7919\nEpoch 21/40\n782/782 [==============================] - 243s 310ms/step - loss: 0.7357 - accuracy: 0.7444 - val_loss: 0.7494 - val_accuracy: 0.7649\nEpoch 22/40\n663/782 [========================>.....] - ETA: 35s - loss: 0.7259 - accuracy: 0.7473wandb: Network error (ConnectTimeout), entering retry loop.\n782/782 [==============================] - 243s 311ms/step - loss: 0.7269 - accuracy: 0.7468 - val_loss: 0.7443 - val_accuracy: 0.7634\nEpoch 23/40\n782/782 [==============================] - ETA: 0s - loss: 0.7221 - accuracy: 0.7532WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 245s 314ms/step - loss: 0.7221 - accuracy: 0.7532 - val_loss: 0.5855 - val_accuracy: 0.8091\nEpoch 24/40\n782/782 [==============================] - 241s 308ms/step - loss: 0.7087 - accuracy: 0.7542 - val_loss: 0.6543 - val_accuracy: 0.7835\nEpoch 25/40\n782/782 [==============================] - 237s 303ms/step - loss: 0.7052 - accuracy: 0.7564 - val_loss: 0.6590 - val_accuracy: 0.7874\nEpoch 26/40\n782/782 [==============================] - 238s 305ms/step - loss: 0.7015 - accuracy: 0.7560 - val_loss: 0.6636 - val_accuracy: 0.7853\nEpoch 27/40\n782/782 [==============================] - 240s 307ms/step - loss: 0.6973 - accuracy: 0.7608 - val_loss: 0.6137 - val_accuracy: 0.8007\nEpoch 28/40\n782/782 [==============================] - 241s 309ms/step - loss: 0.6904 - accuracy: 0.7603 - val_loss: 0.6559 - val_accuracy: 0.7940\nEpoch 29/40\n782/782 [==============================] - ETA: 0s - loss: 0.6815 - accuracy: 0.7663WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 264s 338ms/step - loss: 0.6815 - accuracy: 0.7663 - val_loss: 0.5554 - val_accuracy: 0.8165\nEpoch 30/40\n782/782 [==============================] - 244s 313ms/step - loss: 0.6766 - accuracy: 0.7654 - val_loss: 0.6758 - val_accuracy: 0.7863\nEpoch 31/40\n782/782 [==============================] - 243s 311ms/step - loss: 0.6701 - accuracy: 0.7658 - val_loss: 0.5876 - val_accuracy: 0.8059\nEpoch 32/40\n782/782 [==============================] - 241s 309ms/step - loss: 0.6732 - accuracy: 0.7700 - val_loss: 0.6069 - val_accuracy: 0.8102\nEpoch 33/40\n782/782 [==============================] - ETA: 0s - loss: 0.6605 - accuracy: 0.7717WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221112_183227-18cp0wz1/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221112_183227-18cp0wz1/files/model-best)... Done. 0.0s\n782/782 [==============================] - 241s 308ms/step - loss: 0.6605 - accuracy: 0.7717 - val_loss: 0.5246 - val_accuracy: 0.8293\nEpoch 34/40\n782/782 [==============================] - 239s 305ms/step - loss: 0.6585 - accuracy: 0.7734 - val_loss: 0.5481 - val_accuracy: 0.8209\nEpoch 35/40\n782/782 [==============================] - 240s 307ms/step - loss: 0.6520 - accuracy: 0.7751 - val_loss: 0.5401 - val_accuracy: 0.8197\nEpoch 36/40\n782/782 [==============================] - 242s 309ms/step - loss: 0.6521 - accuracy: 0.7751 - val_loss: 0.5949 - val_accuracy: 0.8110\nEpoch 37/40\n782/782 [==============================] - 242s 310ms/step - loss: 0.6424 - accuracy: 0.7787 - val_loss: 0.5622 - val_accuracy: 0.8182\nEpoch 38/40\n782/782 [==============================] - 243s 310ms/step - loss: 0.6438 - accuracy: 0.7793 - val_loss: 0.5590 - val_accuracy: 0.8152\nEpoch 39/40\n782/782 [==============================] - 243s 311ms/step - loss: 0.6404 - accuracy: 0.7794 - val_loss: 0.6040 - val_accuracy: 0.8061\nEpoch 40/40\n782/782 [==============================] - 241s 308ms/step - loss: 0.6387 - accuracy: 0.7796 - val_loss: 0.5499 - val_accuracy: 0.8290\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Test the model on unseen data","metadata":{"tags":[],"cell_id":"2c124faf900f4a6e9ec04856df091ac2","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(\n    x_test,\n    y_test,\n    batch_size=BATCH_SIZE,\n    verbose=VERBOSE\n    )\n    \n#track test results on wandb\nwandb.log({\n    \"test_loss\": test_loss, \n    \"test_accuracy\": test_accuracy\n})","metadata":{"tags":[],"cell_id":"e877aeb307e54160a4241923fe96d0f8","source_hash":"f4369a28","execution_start":1668288023744,"execution_millis":9629,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"157/157 [==============================] - 9s 58ms/step - loss: 0.5499 - accuracy: 0.8290\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ba4822a4-198a-4cdb-8280-0ca8d044b999' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_full_width":true,"deepnote_notebook_id":"8adb2d6615d2412b80f177ef7ae270a8","deepnote_execution_queue":[]}}