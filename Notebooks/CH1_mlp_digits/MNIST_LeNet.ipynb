{"cells":[{"cell_type":"markdown","source":"# Train a CNN on the MNIST dataset","metadata":{"tags":[],"cell_id":"0866d53a43f14f07a25ec690e48dcd93","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"The MNIST dataset is a collection of hand-written digits, labelled with their corresponding true digit representation. We want to use a convolutional neural network to recognize hand-written digits. the intuition of the convolutional layer is to slide a set of filters on the input image, each one encoding informtation on a specific feature. These filters will respond to spatial patterns in the image, very much like neurons in the visual cortex.\n\nFurthermore, we will use a specific architecute that alternates convolutional layers to pooling layers, and ends with a fully connected layer with a softmax activation function. \n![Picture title](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/320px-MnistExamples.png)\n","metadata":{"tags":[],"cell_id":"9ece4fe5090e44b4af88bec2988306e8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Import libraries and define symbolic constants","metadata":{"tags":[],"cell_id":"8646d270b9ac452294f95a53910f34d8","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"### Install wandb to keep track of model performance","metadata":{"tags":[],"cell_id":"8b835a432de840aeb3a70c6e23f8b182","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"!pip install --upgrade wandb\n!wandb login ff97f4ffa6b4b35ec56fc229fc572b0ba72ac1fb","metadata":{"tags":[],"cell_id":"bce19c6ae03e43f795d761788b4dee60","source_hash":"10f68b6d","execution_start":1667752202700,"execution_millis":5336,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /root/venv/lib/python3.9/site-packages (0.13.5)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /root/venv/lib/python3.9/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: setproctitle in /root/venv/lib/python3.9/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.9/site-packages (from wandb) (58.1.0)\nRequirement already satisfied: GitPython>=1.0.0 in /root/venv/lib/python3.9/site-packages (from wandb) (3.1.29)\nRequirement already satisfied: shortuuid>=0.5.0 in /root/venv/lib/python3.9/site-packages (from wandb) (1.0.9)\nRequirement already satisfied: six>=1.13.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: pathtools in /root/venv/lib/python3.9/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: psutil>=5.0.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: promise<3,>=2.0 in /root/venv/lib/python3.9/site-packages (from wandb) (2.3)\nRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (3.19.6)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /root/venv/lib/python3.9/site-packages (from wandb) (1.10.1)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: PyYAML in /root/venv/lib/python3.9/site-packages (from wandb) (6.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /root/venv/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /root/venv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Import and initialize parameters","metadata":{"tags":[],"cell_id":"838bd1d9e2ae4746bdce6f20c948d7ef","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\n\nimport wandb\nfrom wandb.keras import WandbCallback\n\nwandb.init(project=\"digit-recognition-leNet\")\n#data-related constants\nIMG_ROWS, IMG_COLUMNS = 28, 28\nINPUT_SHAPE = (IMG_ROWS, IMG_COLUMNS, 1)\nNB_CLASSES = 10 # we have 10 classes in our dataset, hence 10 neurons in the last layer\nVERBOSE = 1 # make it loud\n\n#hyperparameters\nEPOCHS = 20  # this is how many times re-train the model, each time optimizing its weight and biases\nBATCH_SIZE = 128 # this is the number of instances we take from the training set before running the optimizer\nN_FILTERS1 = 20 #filters of the first convolution\nN_FILTERS2 = 80 #filters of the second convolution\nFILTER_SHAPE = (5,5) # shape of the filters in the convolutional layers\nPOOL_SHAPE = (2,2) #shape of the pooling filters in the maxpooling layers\nPOOL_STRIDES = (2,2) # strides of the pooling process \nN_DENSE = 800 #neurons in the dense layer before the softmax\nVALIDATION_SPLIT = 0.90 #leave 90% of the training set out for validation (accuracy, to avoid overfitting)\nACTIVATION_FUNCTION_HIDDEN = 'relu' # activation function for the hidden layers\nACTIVATION_FUNCTION_FINAL = 'softmax' # activation function for the output layer \nOPTIMIZER = 'adam' # optimizer, this is how we search for the minimum in the loss function\nLOSS_FUNCTION = 'categorical_crossentropy' #loss function, this is what is otimized\nMETRICS = ['accuracy'] #Our metrics, used to make sure we don't overfit. Computed also on the test set \n\nwandb.config = {\n  \"epochs\": EPOCHS,\n  \"batch_size\": BATCH_SIZE, \n  \"n_hidden\": N_DENSE,\n  \"validation_split\": VALIDATION_SPLIT,\n  'activation_funciton_hidden': ACTIVATION_FUNCTION_HIDDEN,\n  'activation_funciton_final': ACTIVATION_FUNCTION_FINAL,\n  'optimizer': OPTIMIZER,\n  'loss_function': LOSS_FUNCTION,\n  'metric': METRICS,\n}\n","metadata":{"tags":[],"cell_id":"c0d1e06a90a843f0b34d6a94cc2f264a","source_hash":"16deb46c","execution_start":1667752208083,"execution_millis":8120,"is_output_hidden":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-11-06 16:30:08.058815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-06 16:30:08.186737: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-11-06 16:30:08.192488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-11-06 16:30:08.192509: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-11-06 16:30:08.216194: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-11-06 16:30:10.174361: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-11-06 16:30:10.174433: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-11-06 16:30:10.174442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicklerick\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.5"},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/work/wandb/run-20221106_163015-37cc3uiz</code>"},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/picklerick/digit-recognition-leNet/runs/37cc3uiz\" target=\"_blank\">dazzling-lake-7</a></strong> to <a href=\"https://wandb.ai/picklerick/digit-recognition-leNet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{},"output_type":"display_data"}],"execution_count":2},{"cell_type":"markdown","source":"## Load demo dataset from Keras\r","metadata":{"tags":[],"cell_id":"88d44aeb993d4119a687ebd89c314442","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"mnist = keras.datasets.mnist\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()","metadata":{"tags":[],"cell_id":"41d0f404cf454e109d46627c03d8e72a","source_hash":"24f9ab52","execution_start":1667752216204,"execution_millis":257,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Reshape data and encode labels (one-hot)","metadata":{"tags":[],"cell_id":"5a52005a590f44a5b7ce25fc120e7b32","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"X_train = X_train.reshape(60000, IMG_ROWS, IMG_COLUMNS, 1).astype(\"float32\")/255\nX_test = X_test.reshape(10000, IMG_ROWS, IMG_COLUMNS, 1).astype(\"float32\")/255\n\n# use a One-hot representaiton for the digits\nY_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\nY_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)","metadata":{"tags":[],"cell_id":"de8fec210cf247d5a0d697e16c698e60","source_hash":"d946728f","execution_start":1667752216464,"execution_millis":64,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Build the model","metadata":{"tags":[],"cell_id":"340319b4b9d54b7a8d9c34045d4a92ab","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"- The model alternates two layers of convolution, relu, pooling","metadata":{"tags":[],"cell_id":"d1f065457f1e459bb1de6f14ff57755a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Followed by flattening, a dense layer and a logistic regression (softmax with output N_classes)","metadata":{"tags":[],"cell_id":"c7dc57be620c40a885cedad8ed574ce9","is_collapsed":false,"formattedRanges":[{"url":"https://deepai.org/machine-learning-glossary-and-terms/softmax-layer","type":"link","ranges":[],"toCodePoint":95,"fromCodePoint":95}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"class LeNet():\n    def build(input_shape, number_of_classes):\n        model = tf.keras.models.Sequential()\n        # conv -> relu -> pool\n        model.add( \n            keras.layers.Convolution2D(#convolution\n            N_FILTERS1,#50 neurons\n            FILTER_SHAPE,\n            activation=ACTIVATION_FUNCTION_HIDDEN,# with relu activation function\n            input_shape=input_shape\n            )\n        )\n        model.add(\n            keras.layers.MaxPooling2D(\n                pool_size=POOL_SHAPE,#pooling\n                strides=POOL_STRIDES,\n            )\n        )\n        # conv -> relu -> pool\n        model.add( \n            keras.layers.Convolution2D( #convolution\n            N_FILTERS2,#more filters in the innermost layer, this is common practive in CNNs\n            FILTER_SHAPE,\n            activation=ACTIVATION_FUNCTION_HIDDEN,# with relu activation function\n            )\n        )\n        model.add(\n            keras.layers.MaxPooling2D(\n                pool_size=POOL_SHAPE, #pooling\n                strides=POOL_STRIDES,\n            )\n        )\n        # flatten -> relu -> softmax\n        model.add(keras.layers.Flatten())\n        model.add(\n            keras.layers.Dense(\n            N_DENSE,\n            activation=ACTIVATION_FUNCTION_HIDDEN,\n            )\n        )\n        model.add(#this is the softmax classifier, or logistic regression\n            keras.layers.Dense(\n            number_of_classes,\n            activation=ACTIVATION_FUNCTION_FINAL,\n            )\n        )\n        return model\n\nmodel = LeNet.build(\n    input_shape=INPUT_SHAPE,\n    number_of_classes=NB_CLASSES\n    )","metadata":{"tags":[],"cell_id":"0f1b27c4b79845e4bb88c26428098908","source_hash":"e2acce40","execution_start":1667752216535,"execution_millis":106,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-11-06 16:30:16.535845: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-11-06 16:30:16.535872: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-11-06 16:30:16.535886: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-ba4822a4-198a-4cdb-8280-0ca8d044b999): /proc/driver/nvidia/version does not exist\n2022-11-06 16:30:16.536108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Compile the model","metadata":{"tags":[],"cell_id":"945258f75b554a1dbcd68e4e4c5e9641","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"- We use stochastic gradient descent","metadata":{"tags":[],"cell_id":"ee1df8dc242d40c293bc423f38ea99cd","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- The loss function is categorical cross-entropy, this is particularly well-suited for multi-class problems with a one-hot encoding ","metadata":{"tags":[],"cell_id":"2829a2f2813d498eb44a2f4d4205621b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We use accuracy to evaluate the performance of the model","metadata":{"tags":[],"cell_id":"cb81110981414e0f90b80d233769b08b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"model.compile(\n    optimizer=OPTIMIZER,\n    loss=LOSS_FUNCTION,\n    metrics=METRICS\n)\n\nmodel.summary()","metadata":{"tags":[],"cell_id":"88eaabded454454b9857f2184d54edd1","source_hash":"1f842d67","execution_start":1667752216687,"execution_millis":17,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 24, 24, 20)        520       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 12, 12, 20)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 80)          40080     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 4, 4, 80)         0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 1280)              0         \n                                                                 \n dense (Dense)               (None, 800)               1024800   \n                                                                 \n dense_1 (Dense)             (None, 10)                8010      \n                                                                 \n=================================================================\nTotal params: 1,073,410\nTrainable params: 1,073,410\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Train the model","metadata":{"tags":[],"cell_id":"a6c23a8054634e0287b36a8bd7cd1fbd","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"We are now ready to train the model. We need to define the number of epochs and the batch size. ","metadata":{"tags":[],"cell_id":"36c8d7aacb9347e6aaa954a256804565","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- Epochs are the number of times the model is exposed to the training dataset. Each time, it will run the optimizer (SGD) and try to minimize the loss function. ","metadata":{"tags":[],"cell_id":"9ca8e36acd35469d9909d2a4faaf1115","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":7,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Batch_size is the number of instances that the optimizer observes before tuning the weights and biases. There are many batches per epoch.","metadata":{"tags":[],"cell_id":"0396da7afb824686a8b3d298eaca9cde","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":10,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We split the training data in an 80% training and 20% validation per epoch. The validation set is used to compute the metric and tune hyperparameters, to avoid overfitting.","metadata":{"tags":[],"cell_id":"eece6fb6ec3748eda85f2ebcf2f9175e","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- We add early stopping, on the loss function on the validation set, with a patience of N epoch. This will stop the optimization if the loss function does not go down for N  consecutive epochs. ","metadata":{"tags":[],"cell_id":"5d52be6f7fca4b409d0b2b37ce01bd59","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"history = model.fit(\n    X_train, \n    Y_train, \n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=VERBOSE,\n    validation_split=VALIDATION_SPLIT,\n    callbacks=[\n        WandbCallback(),\n        ],\n    )","metadata":{"tags":[],"cell_id":"91ec60d9f09d4f88875e7675b7b1caa9","source_hash":"75a65e1d","execution_start":1667752216751,"execution_millis":199770,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\nEpoch 1/20\n46/47 [============================>.] - ETA: 0s - loss: 0.7098 - accuracy: 0.7959WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221106_163015-37cc3uiz/files/model-best)... Done. 0.1s\n47/47 [==============================] - 12s 244ms/step - loss: 0.7002 - accuracy: 0.7980 - val_loss: 0.2314 - val_accuracy: 0.9297\nEpoch 2/20\n46/47 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9484WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221106_163015-37cc3uiz/files/model-best)... Done. 0.1s\n47/47 [==============================] - 11s 237ms/step - loss: 0.1617 - accuracy: 0.9488 - val_loss: 0.1494 - val_accuracy: 0.9549\nEpoch 3/20\n47/47 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9702WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221106_163015-37cc3uiz/files/model-best)... Done. 0.1s\n47/47 [==============================] - 11s 231ms/step - loss: 0.0961 - accuracy: 0.9702 - val_loss: 0.1146 - val_accuracy: 0.9650\nEpoch 4/20\n47/47 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9787WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221106_163015-37cc3uiz/files/model-best)... Done. 0.1s\n47/47 [==============================] - 11s 231ms/step - loss: 0.0690 - accuracy: 0.9787 - val_loss: 0.1145 - val_accuracy: 0.9628\nEpoch 5/20\n47/47 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9852WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\nINFO:tensorflow:Assets written to: /work/wandb/run-20221106_163015-37cc3uiz/files/model-best/assets\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/work/wandb/run-20221106_163015-37cc3uiz/files/model-best)... Done. 0.1s\n47/47 [==============================] - 11s 233ms/step - loss: 0.0503 - accuracy: 0.9852 - val_loss: 0.0833 - val_accuracy: 0.9734\nEpoch 6/20\n47/47 [==============================] - 10s 217ms/step - loss: 0.0346 - accuracy: 0.9892 - val_loss: 0.1305 - val_accuracy: 0.9591\nEpoch 7/20\n47/47 [==============================] - 10s 208ms/step - loss: 0.0289 - accuracy: 0.9920 - val_loss: 0.1213 - val_accuracy: 0.9651\nEpoch 8/20\n47/47 [==============================] - 10s 209ms/step - loss: 0.0241 - accuracy: 0.9922 - val_loss: 0.0894 - val_accuracy: 0.9741\nEpoch 9/20\n47/47 [==============================] - 10s 209ms/step - loss: 0.0180 - accuracy: 0.9948 - val_loss: 0.0863 - val_accuracy: 0.9753\nEpoch 10/20\n47/47 [==============================] - 10s 209ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.0891 - val_accuracy: 0.9755\nEpoch 11/20\n47/47 [==============================] - 10s 208ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.1088 - val_accuracy: 0.9701\nEpoch 12/20\n47/47 [==============================] - 10s 209ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.0881 - val_accuracy: 0.9759\nEpoch 13/20\n47/47 [==============================] - 10s 207ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.0889 - val_accuracy: 0.9770\nEpoch 14/20\n47/47 [==============================] - 10s 211ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9766\nEpoch 15/20\n47/47 [==============================] - 10s 211ms/step - loss: 7.3622e-04 - accuracy: 1.0000 - val_loss: 0.0886 - val_accuracy: 0.9783\nEpoch 16/20\n47/47 [==============================] - 9s 205ms/step - loss: 5.1244e-04 - accuracy: 1.0000 - val_loss: 0.0901 - val_accuracy: 0.9785\nEpoch 17/20\n47/47 [==============================] - 10s 207ms/step - loss: 3.7749e-04 - accuracy: 1.0000 - val_loss: 0.0908 - val_accuracy: 0.9786\nEpoch 18/20\n47/47 [==============================] - 10s 209ms/step - loss: 3.1785e-04 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9787\nEpoch 19/20\n47/47 [==============================] - 10s 208ms/step - loss: 2.8840e-04 - accuracy: 1.0000 - val_loss: 0.0943 - val_accuracy: 0.9786\nEpoch 20/20\n47/47 [==============================] - 10s 205ms/step - loss: 2.4704e-04 - accuracy: 1.0000 - val_loss: 0.0956 - val_accuracy: 0.9786\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Test the model on unseen data","metadata":{"tags":[],"cell_id":"852673c105a54a3a9e66b1bce9d5a4b8","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n#track test results on wandb\nwandb.log({\n    \"test_loss\": test_loss, \n    \"test_accuracy\": test_accuracy\n})","metadata":{"tags":[],"cell_id":"3db67c819d8749f3a64636b2f6ca53d4","source_hash":"76b17787","execution_start":1667752416535,"execution_millis":2001,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"313/313 [==============================] - 2s 6ms/step - loss: 0.0700 - accuracy: 0.9831\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ba4822a4-198a-4cdb-8280-0ca8d044b999' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_full_width":true,"deepnote_notebook_id":"f4d2b712fb5142bd9b98f6bbdd0f08d2","deepnote_execution_queue":[]}}